replicaCount: 1

deploymentStrategy:
 priorityClassName: "high-priority"

#securityContext:
#  fsGroup: 0
#  runAsUser: 0
#  runAsGroup: 0
#  allowPrivilegeEscalation: true
#  privileged: true
#
initSysctl:
  image: busybox:1.36

initFs:
  enabled: true
  Image: busybox:1.36
#
#containerSecurityContext:
#  allowPrivilegeEscalation: false
#  runAsNonRoot: true
#  runAsUser: 1000
#  runAsGroup: 0
#  seccompProfile:
#    type: RuntimeDefault
#  capabilities:
#    drop: ["ALL"]
#  readOnlyRootFilesystem: true
#
#initContainers:
#  image: busybox:1.36
#  securityContext:
#    allowPrivilegeEscalation: false
#    runAsNonRoot: true
#    runAsUser: 1000
#    runAsGroup: 0
#    seccompProfile:
#      type: RuntimeDefault
#    capabilities:
#      drop: ["ALL"]
#    readOnlyRootFilesystem: true

elasticsearch:
  # DEPRECATED: Use initSysctl.enabled instead
  configureNode: false # default false
  bootstrapChecks: true # default true

service:
  type: ClusterIP
  externalPort: 9000
  internalPort: 9000
  labels:
  annotations: {}

#tolerations:
# - key: "sonarqube"
#   operator: "Equal"
#   value: "true"
#   effect: "NoSchedule"

#nodeSelector:
#  sonarqube: "true"

nodeSelector:
  role: "django" # assigning app nodegroup for POC purposes

readinessProbe: # http://localhost: # http://sonarqube-sonarqube.sonarqube.svc.cluster.local
  exec:
    command:
    - sh
    - -c
    - |
      #!/bin/bash
      # A Sonarqube container is considered ready if the status is UP, DB_MIGRATION_NEEDED or DB_MIGRATION_RUNNING
      # status about migration are added to prevent the node to be kill while sonarqube is upgrading the database.
      if wget --no-proxy -qO- http://localhost:{{ .Values.service.internalPort }}{{ .Values.readinessProbe.sonarWebContext | default (include "sonarqube.webcontext" .) }}api/system/status | grep -q -e '"status":"UP"' -e '"status":"DB_MIGRATION_NEEDED"' -e '"status":"DB_MIGRATION_RUNNING"'; then
        exit 0
      fi
      exit 1

livenessProbe: # http://localhost: # http://sonarqube-sonarqube.sonarqube.svc.cluster.local
  exec:
    command:
    - sh
    - -c
    - |
      wget --no-proxy --quiet -O /dev/null --timeout={{ .Values.livenessProbe.timeoutSeconds }} --header="X-Sonar-Passcode: $SONAR_WEB_SYSTEMPASSCODE" "http://localhost:{{ .Values.service.internalPort }}{{ .Values.livenessProbe.sonarWebContext | default (include "sonarqube.webcontext" .) }}api/system/liveness"
  initialDelaySeconds: 60

## Environment variables to attach to the pods
##
# env:
#   # If you use a different ingress path from /, you have to add it here as the value of SONAR_WEB_CONTEXT
#   - name: SONAR_WEB_CONTEXT
#     value: /sonarqube
#   - name: VARIABLE
#     value: my-value

## Those default are based on the default `Web -Xmx1G -Xms128m` and `CE -Xmx2G -Xms128m` and Search -Xmx2G -Xms2G settings of SQ sub processes
## Adjust these values to your needs, you can find more details on the main README of the chart.
# SONAR_CE_JAVAOPTS # sonar.ce.javaOpts
resources:
  limits:
    cpu: 800m
    memory: 6144M
    ephemeral-storage: 5536M
  requests:
    cpu: 400m
    memory: 2048M # 1536M
    ephemeral-storage: 1536M

# SONAR_WEB_JAVAOPTS= #The web server is executed in a dedicated Java process. Use this property to customize JVM options.
# SONAR_WEB_JAVAADDITIONALOPTS= #Same as previous property, but allows to not repeat all other settings like -Xmx

# A custom sonar.properties file can be provided via dictionary.
# For example:

#sonarProperties:
##  #sonar.forceAuthentication: true
#  sonar.web.host: sonarqube-sonarqube.sonarqube.svc.cluster.local
#  sonar.web.port: 9000
#  sonar.host.url: http://sonarqube-sonarqube.sonarqube.svc.cluster.local:9000

# Additional sonar properties to load from a secret with a key "secret.properties" (must be a string)
# sonarSecretProperties:

# Kubernetes secret that contains the encryption key for the sonarqube instance.
# The secret must contain the key 'sonar-secret.txt'.
# The 'sonar.secretKeyPath' property will be set automatically.
# sonarSecretKey: "settings-encryption-secret"

tests:
  enabled: false

serviceAccount:
  create: false
  name: sonarqube
  automountToken: true

#account:
#  # values can be also provided by a secret that contains "password" and "currentPassword" as keys. You can generate such a secret in your cluster
#  # using "kubectl create secret generic admin-password-secret-name --from-literal=password=admin --from-literal=currentPassword=admin"
#  adminPasswordSecretName: "sonarqube-secrets"

postgresql:
  enabled: false # creates `sample` db, doesn't really work with chart. RDS is used instead

## Override JDBC values
## for external Databases
jdbcOverwrite:
  #enable: true # check
  enabled: true
  jdbcUrl: "jdbc:postgresql://django-production-rds-sonar.cfwo68mist26.us-east-1.rds.amazonaws.com:5432/postgres?socketTimeout=1500" # The JDBC url of the external DB
  #jdbcUrl: 'jdbc:postgresql://<my_DB_endpoint>.us-east-1.rds.amazonaws.com/sonar_sandbox?socketTimeout=1500'
  jdbcUsername: "sonarqube" # The DB user that should be used for the JDBC connection # check
  jdbcPassword: "Ve3-?2LlMp2V-hiFkt0eh&e&gMl," # # Use this if you don't mind the DB password getting stored in plain text within the values file
  #jdbcSecretName: "sonarqube-secrets" # pre-existing k8s secret containing the DB password
  #jdbcSecretPasswordKey: "sonar_rds_password" # `"jdbc-password"` secretValueKey of the password found within that secret

#extraConfig: # contains JDBC/RDS connection values
#  secrets:
#    - sonarqube-secrets